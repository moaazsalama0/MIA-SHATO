services:
  ui:
    build: ./ui-service
    ports:
      - "7860:7860"
    depends_on:
      - orchestrator

  orchestrator:
    build: ./orchestrator-api
    ports:
      - "8000:8000"
    depends_on:
      - stt
      - llm
      - validator
      - tts

  stt:
    build: ./stt-service
    ports:
      - "8003:8003"

  validator:
    build: ./robot-validator-api
    ports:
      - "8001:8001"

  tts:
    build: ./tts-service
    ports:
      - "8004:8004"
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"   # expose Ollama API
    volumes:
      - ollama_models:/root/.ollama   # persist downloaded models
    restart: unless-stopped
  llm:
    build: ./llm-service
    container_name: llm-service
    ports:
      - "9000:9000"     # expose your FastAPI service
    environment:
      - OLLAMA_URL=http://ollama:11434/api/generate   # use service name, not localhost
    depends_on:
      - ollama
    restart: unless-stopped

volumes:
  ollama_models: